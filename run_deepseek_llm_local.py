#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# -----------------------------
# é…ç½®
# -----------------------------
# æœ¬åœ° tokenizer è·¯å¾„ï¼ˆåŸå§‹ deepseek-r1:7bï¼‰
TOKENIZER_PATH = "deepseek-r1-7b-tokenizer"

# LoRA åˆå¹¶åçš„æ¨¡å‹è·¯å¾„
MERGED_MODEL_PATH = "merged-model"

# ç”Ÿæˆæ–‡æœ¬æœ€å¤§é•¿åº¦
MAX_NEW_TOKENS = 128

# -----------------------------
# è®¾å¤‡é€‰æ‹©
# -----------------------------
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

# -----------------------------
# åŠ è½½ tokenizer å’Œæ¨¡å‹
# -----------------------------
print("âœ… åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)
print("âœ… tokenizer åŠ è½½å®Œæˆ")

print("âœ… åŠ è½½åˆå¹¶åçš„ LoRA æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MERGED_MODEL_PATH,
    device_map=device,
    torch_dtype=torch.float16
)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# -----------------------------
# å°è£… pipeline
# -----------------------------
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map=device
)

# -----------------------------
# æŸ¥è¯¢å‡½æ•°
# -----------------------------
def query_model(prompt: str):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# -----------------------------
# ä¸»ç¨‹åº
# -----------------------------
if __name__ == "__main__":
    print("\nâœ… æ¨¡å‹å‡†å¤‡å°±ç»ªï¼Œå¯ä»¥è¾“å…¥é—®é¢˜")
    while True:
        prompt = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆexité€€å‡ºï¼‰: ").strip()
        if prompt.lower() == "exit":
            break
        answer = query_model(prompt)
        print(f"\nğŸ’¡ å›ç­”:\n{answer}")
