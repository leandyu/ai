import os
import re
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

vector_db_dir = os.path.abspath("vector_db")
embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

db = Chroma(persist_directory=vector_db_dir, embedding_function=embedding_model)

print(f"âœ… Document count after insert: {db._collection.count()}")

# æŸ¥è¯¢
query = "è¯·æ‰¾å‡ºåŒ…å«äººç‰©æ²ˆè€äºŒçš„æè¿°"

print("\n=== ğŸ” æœ€ç›¸å…³ç»“æœ ===")
results = db.similarity_search(query, k=3)
for i, doc in enumerate(results):
    print(f"\nResult {i+1}: {doc.page_content}")


print("\n=== ğŸ” æœ€ç›¸å…³ç»“æœï¼ˆå¥å­çº§ï¼‰ ===")
# ä½¿ç”¨ MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰ï¼Œk=10 ä¿ç•™æ›´å¤šå€™é€‰
results = db.max_marginal_relevance_search(query, k=10, fetch_k=20)

# æå–å’Œ query ç›¸å…³çš„å¥å­
def extract_relevant_sentences(text, query, top_n=2):
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ\n]", text)
    scores = [(s, len(set(query) & set(s))) for s in sentences if s.strip()]
    scores.sort(key=lambda x: x[1], reverse=True)
    return [s for s, _ in scores[:top_n]]

for i, doc in enumerate(results):
    relevant_sentences = extract_relevant_sentences(doc.page_content, query)
    print(f"\nResult {i+1}:")
    for sent in relevant_sentences:
        print(f"  - {sent}")