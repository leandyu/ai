#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import HuggingFacePipeline

# -----------------------------
# é…ç½®
# -----------------------------
# åŸå§‹ deepseek tokenizer
BASE_MODEL = "deepseek-r1:7b"

# LoRA åˆå¹¶åçš„æ¨¡å‹è·¯å¾„
MERGED_MODEL_PATH = "merged-model"

# å‘é‡æ•°æ®åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
VECTOR_DB_DIR = "./vector_db"

# ç”Ÿæˆæ–‡æœ¬æœ€å¤§é•¿åº¦
MAX_NEW_TOKENS = 128

# -----------------------------
# è®¾å¤‡é€‰æ‹©
# -----------------------------
device = "mps" if torch.backends.mps.is_available() else "cpu"
print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")

# -----------------------------
# åŠ è½½ tokenizer å’Œæ¨¡å‹
# -----------------------------
print("âœ… åŠ è½½ tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
print("âœ… tokenizer åŠ è½½å®Œæˆ")

print("âœ… åŠ è½½åˆå¹¶åçš„ LoRA æ¨¡å‹...")
model = AutoModelForCausalLM.from_pretrained(
    MERGED_MODEL_PATH,
    device_map=device,
    torch_dtype=torch.float16
)
print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

# -----------------------------
# å°è£… pipeline
# -----------------------------
pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map=device
)
llm = HuggingFacePipeline(pipeline=pipe)

# -----------------------------
# åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
# -----------------------------
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectordb = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embeddings)

# -----------------------------
# æŸ¥è¯¢å‡½æ•°
# -----------------------------
def query_model(prompt: str, use_vectordb: bool = False):
    if use_vectordb:
        docs = vectordb.similarity_search(prompt, k=3)
        context = "\n".join([doc.page_content for doc in docs])
        full_prompt = f"Context:\n{context}\n\nQuestion: {prompt}"
    else:
        full_prompt = prompt

    inputs = tokenizer(full_prompt, return_tensors="pt").to(device)
    outputs = model.generate(**inputs, max_new_tokens=MAX_NEW_TOKENS)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

# -----------------------------
# ä¸»ç¨‹åº
# -----------------------------
if __name__ == "__main__":
    while True:
        prompt = input("\nè¯·è¾“å…¥é—®é¢˜ï¼ˆexité€€å‡ºï¼‰: ").strip()
        if prompt.lower() == "exit":
            break
        answer = query_model(prompt)
        print(f"\nğŸ’¡ å›ç­”:\n{answer}")
